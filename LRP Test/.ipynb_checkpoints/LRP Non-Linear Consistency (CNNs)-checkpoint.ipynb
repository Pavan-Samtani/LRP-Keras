{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tfUtils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import innvestigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Activation, Dense, Flatten, ZeroPadding2D, Conv2D, BatchNormalization, MaxPooling2D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(m, n_x, seed = 9):\n",
    "    np.random.seed(seed)\n",
    "    X = np.random.randn(m, n_x)\n",
    "    Y = np.zeros((m, 2))\n",
    "    dist = np.random.randint(low = 0, high = n_x-1, size = m)\n",
    "    for i in range(m):\n",
    "        t = np.random.rand();\n",
    "        if t < 0.5:\n",
    "            X[i, dist[i]] = -20\n",
    "            Y[i, 0] = 1\n",
    "        elif t >= 0.5:\n",
    "            X[i, dist[i]] = 20\n",
    "            Y[i, 1] = 1\n",
    "    return X, Y, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(shape, name, which):\n",
    "    if which == \"weights\":\n",
    "        return tf.get_variable(name, shape, \n",
    "                               initializer = tf.contrib.layers.xavier_initializer())\n",
    "    elif which == \"bias\":\n",
    "        return tf.get_variable(name, shape, \n",
    "                              initializer = tf.zeros_initializer())\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_h, n_w, n_c, n_y):\n",
    "    X = tf.placeholder(tf.float32, [None, n_h, n_w, n_c])\n",
    "    Y = tf.placeholder(tf.float32, [None, n_y])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(layer):\n",
    "    size = layer.get_shape()\n",
    "    num_features = size[1] * size[2] * size[3]\n",
    "    flattened_layer = tf.reshape(layer, [-1, num_features])\n",
    "    return flattened_layer, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(layer, input_features, output_features, activation, indicator):\n",
    "    weights = initialize_parameters([input_features, output_features], \"W\" + str(indicator), \"weights\")\n",
    "    bias = initialize_parameters([1, output_features], \"b\" + str(indicator), \"bias\")\n",
    "    Z = tf.matmul(layer, weights) + bias\n",
    "    if activation == \"relu\":\n",
    "        return tf.nn.relu(Z), weights, bias\n",
    "    elif activation == \"sigmoid\":\n",
    "        return tf.nn.sigmoid(Z), weights, bias\n",
    "    elif activation == \"tanh\":\n",
    "        return tf.nn.tanh(Z), weights, bias\n",
    "    elif activation == \"linear\":\n",
    "        return Z, weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_2D(layer, filter_size, output_channels, stride, padding, activation, use_maxpool, indicator):\n",
    "    input_channels = layer.get_shape()[3]\n",
    "    weights = initialize_parameters([filter_size, filter_size, input_channels, output_channels], \"W\" + str(indicator), \n",
    "                                   \"weights\")\n",
    "    bias = initialize_parameters([output_channels], \"b\" + str(indicator), \"bias\")\n",
    "    Z = tf.nn.conv2d(layer, weights, strides = [1, stride, stride, 1], padding = padding) + bias\n",
    "    if activation == \"relu\":\n",
    "        A = tf.nn.relu(Z)\n",
    "    elif activation == \"sigmoid\":\n",
    "        A = tf.nn.sigmoid(Z)\n",
    "    elif activation == \"tanh\":\n",
    "        A = tf.nn.tanh(Z)\n",
    "    elif activation == \"linear\":\n",
    "        A = Z\n",
    "    if use_maxpool:\n",
    "        return tf.nn.max_pool(A, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'VALID'), weights, bias\n",
    "    else:\n",
    "        return A, weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z, Y):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z, labels = Y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, dist = generate_data(50000, 144)\n",
    "X_train = X_train.reshape(-1, 12, 12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, layers, filter_size, stride, padding, activation,\n",
    "          epochs, batch_size, starting_rate, decay, use_maxpool):\n",
    "    tf.reset_default_graph()\n",
    "    input_shape = X_train.shape\n",
    "    output_shape = Y_train.shape\n",
    "    X, Y = create_placeholders(input_shape[1], input_shape[2], input_shape[3], output_shape[1])\n",
    "    parameters = {}\n",
    "    A = X\n",
    "    already_flat = 0\n",
    "    for i in range(1, len(layers) + 1):\n",
    "        if layers[i - 1][0] == 'conv_2D':\n",
    "            A, parameters['W' + str(i)], parameters['b' + str(i)] = conv_2D(\n",
    "                A, filter_size, layers[i - 1][1], stride, padding, activation[i - 1], use_maxpool, indicator = i)\n",
    "        elif layers[i - 1][0] == 'fc':\n",
    "            if already_flat == 0:\n",
    "                A, input_layers = flatten(A)\n",
    "                already_flat = 1\n",
    "            else: input_layers = A.get_shape()[1]\n",
    "            A, parameters['W' + str(i)], parameters['b' + str(i)] = fully_connected(\n",
    "                A, input_layers, layers[i - 1][1], activation[i - 1], indicator = i)\n",
    "    cost = compute_cost(A, Y)\n",
    "    hard_A = tf.argmax(tf.nn.softmax(A, axis = 1), axis = 1)\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(hard_A, tf.argmax(Y, axis = 1)), tf.float32))\n",
    "    global_steps = tf.Variable(0, trainable = False)\n",
    "    learning_rate = tf.train.exponential_decay(starting_rate, global_steps, 5000, decay, staircase = True)\n",
    "    train = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost, global_step = global_steps)\n",
    "    init = tf.global_variables_initializer()\n",
    "    cost_list = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(epochs):\n",
    "            minibatches = random_minibatches(X_train, Y_train, batch_size, seed = epoch)\n",
    "            epoch_cost = 0\n",
    "            for minibatch in minibatches:\n",
    "                sess.run(train, feed_dict = {X : minibatch[0], Y : minibatch[1]})\n",
    "                epoch_cost += sess.run(cost, feed_dict = {X : minibatch[0], Y : minibatch[1]}) / len(minibatches)\n",
    "            cost_list.append(epoch_cost)        \n",
    "            if epoch % 5 == 0:\n",
    "                print(epoch_cost)\n",
    "        parameter_names = []\n",
    "        parameter_list = []\n",
    "        for i in range(1, len(parameters) // 2 + 1):\n",
    "            parameter_names.append(\"W\" + str(i))\n",
    "            parameter_names.append(\"b\" + str(i))\n",
    "        for j in range(len(parameter_names)):\n",
    "            parameter_list.append(sess.run(parameters[parameter_names[j]]))\n",
    "        print(\"accuracy\", sess.run(acc, feed_dict = {X : X_train, Y : Y_train}))\n",
    "    sess.close()\n",
    "    plt.plot(np.array(cost_list), '-b')\n",
    "    plt.show()\n",
    "    return parameter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-a25a04e7d6f0>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "0.2180096482785368\n",
      "0.20378436575156922\n"
     ]
    }
   ],
   "source": [
    "parameter_list_relu = model(X_train, Y_train, layers = [(\"conv_2D\", 32), (\"conv_2D\", 64), (\"fc\", 512), (\"fc\", 1024), (\"fc\", 2)], \n",
    "                           filter_size = 3, stride = 1, padding = 'VALID', \n",
    "                            activation = [\"relu\", \"relu\", \"relu\", \"relu\", \"linear\"], epochs = 100, batch_size = 32, \n",
    "                            starting_rate = 3e-4, decay = .9, use_maxpool = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_list_tanh = model(X_train, Y_train, layers = [(\"conv_2D\", 32), (\"conv_2D\", 64), (\"fc\", 512), (\"fc\", 1024), (\"fc\", 2)], \n",
    "                           filter_size = 3, stride = 1, padding = 'VALID', \n",
    "                            activation = [\"tanh\", \"tanh\", \"tanh\", \"tanh\", \"linear\"], epochs = 60, batch_size = 32, \n",
    "                            starting_rate = 5e-4, decay = .85, use_maxpool = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_list_sigmoid = model(X_train, Y_train, layers = [(\"conv_2D\", 32), (\"conv_2D\", 64), (\"fc\", 512), (\"fc\", 1024), (\"fc\", 2)], \n",
    "                           filter_size = 3, stride = 1, padding = 'VALID', \n",
    "                            activation = [\"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\", \"linear\"], epochs = 100, batch_size = 16, \n",
    "                            starting_rate = 1e-4, decay = .9, use_maxpool = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, activations, use_soft):\n",
    "    X_input = Input(input_shape);\n",
    "    X = Conv2D(32, (3,3), strides = (1,1), padding = \"valid\", name = \"Z_1\")(X_input);\n",
    "    X = Activation(activations[0])(X);\n",
    "    X = MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = \"valid\", name = \"max_pool_1\")(X);\n",
    "    X = Conv2D(64, (3,3), strides = (1,1), padding = \"valid\", name = \"Z_2\")(X);\n",
    "    X = Activation(activations[1])(X);\n",
    "    X = MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = \"valid\", name = \"max_pool_2\")(X);\n",
    "    X = Flatten()(X);\n",
    "    X = Dense(512, activation = activations[2], name = \"A_3\")(X)\n",
    "    X = Dense(1024, activation = activations[3], name = \"A_4\")(X)\n",
    "    if use_soft:\n",
    "        X = Dense(2, activation = \"softmax\", name = \"A_5\")(X)\n",
    "    else:\n",
    "        X = Dense(2, activation = \"linear\", name = \"Z_5\")(X)\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Model Weights and relevant parameters/CNNs/relu_model_weights.npy', parameter_list_relu)\n",
    "np.save('Model Weights and relevant parameters/CNNs/sigmoid_model_weights.npy', parameter_list_sigmoid)\n",
    "np.save('Model Weights and relevant parameters/CNNs/tanh_model_weights.npy', parameter_list_tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_list_relu = np.load('Model Weights and relevant parameters/CNNs/relu_model_weights.npy')\n",
    "parameter_list_sigmoid = np.load('Model Weights and relevant parameters/CNNs/sigmoid_model_weights.npy')\n",
    "parameter_list_tanh = np.load('Model Weights and relevant parameters/CNNs/tanh_model_weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelp_relu = create_model(X_train.shape[1:], [\"relu\", \"relu\", \"relu\", \"relu\"], True)\n",
    "model_relu = create_model(X_train.shape[1:], [\"relu\", \"relu\", \"relu\", \"relu\"], False)\n",
    "modelp_tanh = create_model(X_train.shape[1:], [\"tanh\", \"tanh\", \"tanh\", \"tanh\"], True)\n",
    "model_tanh = create_model(X_train.shape[1:], [\"tanh\", \"tanh\", \"tanh\", \"tanh\"], False)\n",
    "modelp_sigmoid = create_model(X_train.shape[1:], [\"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\"], True)\n",
    "model_sigmoid = create_model(X_train.shape[1:], [\"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\"], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelp_relu.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "modelp_tanh.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "modelp_sigmoid.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(parameter_list_relu)):\n",
    "    if i % 2 == 1:\n",
    "        parameter_list_relu[i] = parameter_list_relu[i].reshape(-1)\n",
    "        parameter_list_tanh[i] = parameter_list_tanh[i].reshape(-1)\n",
    "        parameter_list_sigmoid[i] = parameter_list_sigmoid[i].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelp_relu.set_weights(parameter_list_relu)\n",
    "model_relu.set_weights(parameter_list_relu)\n",
    "modelp_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "modelp_relu.evaluate(x = X_train, y = Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelp_tanh.set_weights(parameter_list_tanh)\n",
    "model_tanh.set_weights(parameter_list_tanh)\n",
    "modelp_tanh.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "modelp_tanh.evaluate(x = X_train, y = Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelp_sigmoid.set_weights(parameter_list_sigmoid)\n",
    "model_sigmoid.set_weights(parameter_list_sigmoid)\n",
    "modelp_sigmoid.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "modelp_sigmoid.evaluate(x = X_train, y = Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_relu = innvestigate.create_analyzer(\"lrp.alpha_2_beta_1_IB\", model_relu)\n",
    "analyzer_sigmoid = innvestigate.create_analyzer(\"lrp.alpha_2_beta_1\", model_sigmoid)\n",
    "analyzer_tanh = innvestigate.create_analyzer(\"lrp.alpha_2_beta_1\", model_tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_relu = analyzer_relu.analyze(X_train[:, :])\n",
    "analysis_sigmoid = analyzer_sigmoid.analyze(X_train[:, :])\n",
    "analysis_tanh = analyzer_tanh.analyze(X_train[:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_me = np.zeros((len(dist), 144))\n",
    "for i in range(len(dist)):\n",
    "    rel_me[i, dist[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_me = np.reshape(rel_me, (-1, 12, 12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "for i in range(100):\n",
    "    f, axarr = plt.subplots(1, 4, figsize=(12, 12))\n",
    "    \n",
    "    m_relu = np.max(np.abs(analysis_relu[i,:,:,0]))\n",
    "    a_relu = ((analysis_relu[i,:,:,0] / m_relu) + 1) / 2\n",
    "    \n",
    "    m_sigmoid = np.max(np.abs(analysis_sigmoid[i,:,:,0]))\n",
    "    a_sigmoid = ((analysis_sigmoid[i,:,:,0] / m_sigmoid) + 1) / 2\n",
    "    \n",
    "    m_tanh = np.max(np.abs(analysis_tanh[i,:,:,0]))\n",
    "    a_tanh = ((analysis_tanh[i,:,:,0] / m_tanh) + 1) / 2\n",
    "    \n",
    "    fig = axarr[0].imshow(a_relu, vmax = 1, vmin = 0, cmap = \"jet\")\n",
    "    fig = axarr[1].imshow(a_sigmoid, vmax = 1, vmin = 0, cmap = \"jet\")\n",
    "    fig = axarr[2].imshow(a_tanh, vmax = 1, vmin = 0, cmap = \"jet\")\n",
    "    fig = axarr[3].imshow(rel_me[i,:,:], cmap = \"binary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
