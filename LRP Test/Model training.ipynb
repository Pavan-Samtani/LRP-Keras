{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tfUtils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import innvestigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Activation, Dense, Flatten\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(m, n_x, seed = 9):\n",
    "    np.random.seed = seed\n",
    "    X = np.random.randn(m, n_x)\n",
    "    Y = np.zeros((m, 2))\n",
    "    dist = np.random.randint(low = 0, high = n_x-1, size = m)\n",
    "    for i in range(m):\n",
    "        t = np.random.rand();\n",
    "        if t < 0.5:\n",
    "            X[i, dist[i]] = -20\n",
    "            Y[i, 0] = 1\n",
    "        elif t >= 0.5:\n",
    "            X[i, dist[i]] = 20\n",
    "            Y[i, 1] = 1\n",
    "    return X, Y, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, dist = generate_data(40000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    X = tf.placeholder(tf.float32, [None, n_x])\n",
    "    Y = tf.placeholder(tf.float32, [None, n_y])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, layers):\n",
    "    parameters = {}\n",
    "    lay = layers[:]\n",
    "    lay.insert(0, n_x)\n",
    "    for i in range(1, len(layers) + 1):\n",
    "        parameters[\"W\" + str(i)] = tf.get_variable(\"W\" + str(i), [lay[i-1], lay[i]],\n",
    "                                                  initializer = tf.contrib.layers.xavier_initializer())\n",
    "        parameters[\"b\" + str(i)] = tf.get_variable(\"b\" + str(i), [1, lay[i]], \n",
    "                                                  initializer = tf.zeros_initializer())\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(A, W, b, activation):\n",
    "    Z = tf.matmul(A, W) + b\n",
    "    if activation == \"relu\":\n",
    "        A = tf.nn.relu(Z)\n",
    "    elif activation == \"sigmoid\":\n",
    "        A = tf.nn.sigmoid(Z)\n",
    "    elif activation == \"tanh\":\n",
    "        A = tf.nn.tanh(Z)\n",
    "    elif activation == \"linear\":\n",
    "        A = Z\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z, Y):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z, labels = Y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_model(X_train, Y_train, layers, activations, epochs, batch_size, starting_rate, decay, lambd):\n",
    "    tf.reset_default_graph()\n",
    "    m, n_x = X_train.shape\n",
    "    __, n_y = Y_train.shape\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    parameter_list = []\n",
    "    parameters = initialize_parameters(n_x, layers)\n",
    "    A = X\n",
    "    for i in range(1, len(layers) + 1):\n",
    "        A = forward_propagation(A, parameters[\"W\" + str(i)], parameters[\"b\" + str(i)], activations[i - 1])\n",
    "    cost = compute_cost(A, Y)\n",
    "    hard_A = tf.argmax(tf.nn.softmax(A, axis = 1), axis = 1)\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(hard_A,tf.argmax(Y, axis = 1)), tf.float32))\n",
    "    for i in range(1, len(layers) + 1):\n",
    "        cost += lambd * tf.nn.l2_loss(parameters[\"W\" + str(i)])\n",
    "    global_steps = tf.Variable(0, trainable = False)\n",
    "    learning_rate = tf.train.exponential_decay(starting_rate, global_steps, 5000, decay, staircase = True)\n",
    "    train = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost, global_step = global_steps)\n",
    "    init = tf.global_variables_initializer()\n",
    "    cost_list = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(epochs):\n",
    "            minibatches = random_minibatches(X_train, Y_train, batch_size, seed = epoch)\n",
    "            epoch_cost = 0\n",
    "            for minibatch in minibatches:\n",
    "                sess.run(train, feed_dict = {X : minibatch[0], Y : minibatch[1]})\n",
    "                epoch_cost += sess.run(cost, feed_dict = {X : minibatch[0], Y : minibatch[1]}) / len(minibatches)\n",
    "            cost_list.append(epoch_cost)        \n",
    "            if epoch % 5 == 0:\n",
    "                print(epoch_cost)\n",
    "        parameter_names = [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\", \"W4\", \"b4\"]\n",
    "        for j in range(len(parameter_names)):\n",
    "            parameter_list.append(sess.run(parameters[parameter_names[j]]))\n",
    "        print(\"accuracy\", sess.run(acc, feed_dict = {X : X_train, Y : Y_train}))\n",
    "    sess.close()\n",
    "    plt.plot(np.array(cost_list), '-b')\n",
    "    return parameter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [200, 300, 100, 2]\n",
    "activations = [[\"relu\", \"relu\", \"relu\", \"linear\"], [\"sigmoid\", \"sigmoid\", \"sigmoid\", \"linear\"], [\"tanh\", \"tanh\", \"tanh\", \"linear\"]]\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "starting_rate = 0.0003\n",
    "decay = .9\n",
    "lambd = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_list_relu = tf_model(X_train, Y_train, layers, activations[0], epochs, batch_size, starting_rate, decay, lambd)\n",
    "parameter_list_sigmoid = tf_model(X_train, Y_train, layers, activations[1], epochs + 10, batch_size, starting_rate, decay, lambd)\n",
    "parameter_list_tanh = tf_model(X_train, Y_train, layers, activations[2], epochs + 10, batch_size, starting_rate, decay, lambd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = X_train.reshape(-1, 10, 10, 1)\n",
    "Y_train_ = Y_train.reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, layers, activations, use_soft):\n",
    "    X_input = Input(input_shape)\n",
    "    X = Flatten()(X_input)\n",
    "    X = Dense(layers[0], activation = activations[0])(X)\n",
    "    X = Dense(layers[1], activation = activations[1])(X)\n",
    "    X = Dense(layers[2], activation = activations[2])(X)\n",
    "    if use_soft :\n",
    "        X = Dense(layers[3], activation = 'softmax')(X)\n",
    "    else :\n",
    "        X = Dense(layers[3], activation = 'linear')(X)\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu = create_model((10, 10, 1), layers, activations[0], False)\n",
    "modelp_relu = create_model((10, 10, 1), layers, activations[0], True)\n",
    "model_sigmoid = create_model((10, 10, 1), layers, activations[0], False)\n",
    "modelp_sigmoid = create_model((10, 10, 1), layers, activations[0], True)\n",
    "model_tanh = create_model((10, 10, 1), layers, activations[0], False)\n",
    "modelp_tanh = create_model((10, 10, 1), layers, activations[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu.set_weights(parameter_list_relu)\n",
    "modelp_relu.set_weights(parameter_list_relu)\n",
    "modelp_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "modelp_relu.evaluate(x = X_train_, y = Y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sigmoid.set_weights(parameter_list_sigmoid)\n",
    "modelp_sigmoid.set_weights(parameter_list_sigmoid)\n",
    "modelp_sigmoid.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "modelp_sigmoid.evaluate(x = X_train_, y = Y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tanh.set_weights(parameter_list_tanh)\n",
    "modelp_tanh.set_weights(parameter_list_tanh)\n",
    "modelp_tanh.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "modelp_tanh.evaluate(x = X_train_, y = Y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_relu = innvestigate.create_analyzer(\"lrp.alpha_2_beta_1\", model_relu)\n",
    "analyzer_sigmoid = innvestigate.create_analyzer(\"lrp.alpha_2_beta_1\", model_sigmoid)\n",
    "analyzer_tanh = innvestigate.create_analyzer(\"lrp.alpha_2_beta_1\", model_tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_relu = analyzer_relu.analyze(X_train_[0:100,:])\n",
    "analysis_sigmoid = analyzer_sigmoid.analyze(X_train_[0:100,:])\n",
    "analysis_tanh = analyzer_tanh.analyze(X_train_[0:100,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_me = np.zeros((len(dist), 100))\n",
    "for i in range(len(dist)):\n",
    "    rel_me[i, dist[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_me = np.reshape(rel_me, (-1, 10, 10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "for i in range(100):\n",
    "    f, axarr = plt.subplots(1, 4, figsize=(4, 4))\n",
    "    m = np.max(np.abs(analysis[i,:,:,0]))\n",
    "    a_relu = ((analysis_relu[i,:,:,0]/m)+1)/2\n",
    "    a_sigmoid = ((analysis_sigmoid[i,:,:,0]/m)+1)/2\n",
    "    a_tanh = ((analysis_tanh[i,:,:,0]/m)+1)/2\n",
    "    fig = axarr[0].imshow(a_relu, vmax = 1, vmin = 0, cmap = \"jet\")\n",
    "    fig = axarr[1].imshow(a_sigmoid, vmax = 1, vmin = 0, cmap = \"jet\")\n",
    "    fig = axarr[2].imshow(a_tanh, vmax = 1, vmin = 0, cmap = \"jet\")\n",
    "    fig = axarr[3].imshow(rel_me[i,:,:], cmap = \"binary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
